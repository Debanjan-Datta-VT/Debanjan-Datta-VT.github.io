---
layout: page
title: Parameter Efficient Finetuning for LLM
description: AWS Sagemaker Canvas Enterprise Project Roadmap
importance: 1
category: Amazon Web Services
related_publications: true
---

<div class="container-fluid"> 
  <!-- Project Overview Section -->
  <div class="row mb-4">
    <div class="col-12">
      <div class="card">
        <div class="card-body">
          <h2 class="card-title text-center mb-4">Project Overview</h2>
          <p class="lead text-center">
            Advanced research and development project focused on optimizing Parameter Efficient Fine-tuning (PEFT) methods for enterprise-scale Large Language Model deployment in AWS SageMaker Canvas.
          </p>
        </div>
      </div>
    </div>
  </div>

  <!-- Problem Statement Section -->
  <div class="row mb-4">
    <div class="col-md-3 mb-3">
      <div class="d-flex align-items-center justify-content-center h-100">
        <h3 class="text-primary mb-0">Problem Statement</h3>
      </div>
    </div>
    <div class="col-md-9">
      <div class="card h-100">
        <div class="card-body">
          <p class="card-text">
          This research addressed the challenge of determining optimal hyperparameters and training configurations for state-of-the-art (SoTA) Large Language Models when implementing Parameter Efficient Fine-tuning (PEFT) methods. The goal was to establish best practices for fine-tuning these models while maintaining computational efficiency and achieving superior performance across diverse use cases.
          </p>  
        </div>
      </div>
    </div>
  </div>

  <!-- Business Impact Section -->
  <div class="row mb-4">
    <div class="col-md-3 mb-3">
      <div class="d-flex align-items-center justify-content-center h-100">
        <h3 class="text-success mb-0">Business Impact</h3>
      </div>
    </div>
    <div class="col-md-9">
      <div class="card h-100">
        <div class="card-body">
          <p class="card-text">
            Enabled enterprise customers to fine-tune Large Language Models for domain-specific business applications while maintaining optimal cost efficiency and performance. This capability was integrated into AWS SageMaker Canvas, Amazon's managed machine learning service for no-code/low-code model development.
          </p>
        </div>
      </div>
    </div>
  </div>

  <!-- Technical Details Section -->
  <div class="row mb-4">
    <div class="col-md-3 mb-3">
      <div class="d-flex align-items-center justify-content-center h-100">
        <h3 class="text-info mb-0">Technical Details & Contributions</h3>
      </div>
    </div>
    <div class="col-md-9">
      <div class="card h-100">
        <div class="card-body">
          <ul class="list-unstyled">
            <li>Designed and executed large-scale experiments to understand the effects of fine-tuning LLMs using PEFT methods (LoRA, qLoRA), uncovering overall performance envelopes and investigating failure conditions.</li>
            <li>Conducted comprehensive analysis of the hyperparameter landscape for PEFT using multiple methodologies and performance profiling techniques.</li>
            <li>Analyzed performance trade-offs when applying PEFT to domain-specific datasets in resource-constrained environments.</li>
            <li>Experimented with multiple distributed training paradigms including Fully Sharded Data Parallel (FSDP), Distributed Data Parallel (DDP), and Data Parallel (DP).</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- Keywords -->
  <div class="row mb-4">
    <div class="col-md-3 mb-3">
      <div class="d-flex align-items-center justify-content-center h-100">
        <h3 class="text-info mb-0">Keywords</h3>
      </div>
    </div>
    <div class="col-md-9">
      <div class="card h-100">
        <div class="card-body">
          <ul class="list-unstyled">
            <li>Large Language Models</li>
            <li>PEFT</li>
            <li>LoRA</li>
            <li>qLoRA</li>
            <li>AWS SageMaker Canvas</li>
            <li>Distributed Training</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

</div>
